# How-to-pay-attention


## "Attention is all you need" paper implementation.
The Transformer model proposed in the [paper](https://arxiv.org/pdf/1706.03762.pdf) had taken the entire NLP community by6 sti\orm because of it's SOTA peformance in less computation power.

The paper also used a concept known as Attention and showed that only attention mechanism can be used as an interaction between layers to achieve SOTA results. So can we say that ***Any architecture which uses only attention as it's medium of interaction with the corresponding layers can be termed as transformer(maybe a basic one but still)?***  



